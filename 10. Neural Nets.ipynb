{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "In this notebook, we will walk through one main neural nets example. And that is, classifying the infamous MNIST dataset. **If you have no experience with neural nets prior to this notebook, I recommend doing a quick search for an \"intro to neural nets\"**, there are multiple tutorials/blog posts out there and you can choose the one that works for you.\n",
    "\n",
    "Here, we will use the `Flux` package, but if you want to look at other packages I encourage you to look at `Knet.jl` and `TensorFlow.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using MLDatasets, MLUtils\n",
    "using Flux: onehotbatch, argmax, crossentropy, throttle\n",
    "using Base.Iterators: repeated\n",
    "using Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAYtJREFUaAW9wb+LzgEAB+AnPqPUXUZdWUhRlMhksFyZDbIYleVMlH/ATpRJ2WwiFv+DssggkqvrhktnYOAY3v3e7w99nifKoizKoizKoizKoizKoizKoizKoizKoizKoizKoixmuornuIkn+Gt/URZlURYz3cUeHuENvtpflEVZlEVZlEVZlMUMp3DUwk/8sVyURVmUxQzXccTCBjYtF2VRFmUxw6qFH9gyTJRFWZTFRIdwwcJrvDJMlEVZlMVEF3HawpbhoizKoiwmOIyTFnbwwHBRFmVRFhOcxW3s4DE+Gy7KoizKYoJ7WMMOnhknyqIsymKkFXzAZfzGnnGiLMqiLEY6hyvYxkN8Mk6URVmUxQiruI9jeImnxouyKIuyGGgFt3AGb3EHm8aLsiiLshjoPK5hGxv4aJooi7Ioi4Fu4AS+4ZfpoizKoiwGWMdxvMcl7JouyqIsymKJNbzAd6xj1zxRFmVRFkscQLCBd+aLsiiLsljiCw76f6IsyqIsyqIsyqIsyqIsyqLsH5MmL74zpQwVAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAYtJREFUaAW9wb+LzgEAB+AnPqPUXUZdWUhRlMhksFyZDbIYleVMlH/ATpRJ2WwiFv+DssggkqvrhktnYOAY3v3e7w99nifKoizKoizKoizKoizKoizKoizKoizKoizKoizKoixmuornuIkn+Gt/URZlURYz3cUeHuENvtpflEVZlEVZlEVZlMUMp3DUwk/8sVyURVmUxQzXccTCBjYtF2VRFmUxw6qFH9gyTJRFWZTFRIdwwcJrvDJMlEVZlMVEF3HawpbhoizKoiwmOIyTFnbwwHBRFmVRFhOcxW3s4DE+Gy7KoizKYoJ7WMMOnhknyqIsymKkFXzAZfzGnnGiLMqiLEY6hyvYxkN8Mk6URVmUxQiruI9jeImnxouyKIuyGGgFt3AGb3EHm8aLsiiLshjoPK5hGxv4aJooi7Ioi4Fu4AS+4ZfpoizKoiwGWMdxvMcl7JouyqIsymKJNbzAd6xj1zxRFmVRFkscQLCBd+aLsiiLsljiCw76f6IsyqIsyqIsyqIsyqIsyqLsH5MmL74zpQwVAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                      ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = MNIST(split=:train)\n",
    "img = train_data[100].features\n",
    "colorview(Gray, img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From targets, we will create a new output column for each image. These columns will be indicator vectors of where the correct label is.\n",
    "\n",
    "The `onehotbatch` function allows us to create this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469-element DataLoader(::Tuple{Matrix{Float32}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, shuffle=true, batchsize=128)\n",
       "  with first element:\n",
       "  (784×128 Matrix{Float32}, 10×128 OneHotMatrix(::Vector{UInt32}) with eltype Bool,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = train_data.features, train_data.targets\n",
    "\n",
    "# Preprocess\n",
    "x_train = flatten(x_train)\n",
    "y_train = onehotbatch(y_train, 0:9)\n",
    "\n",
    "loader = DataLoader((x_train, y_train), batchsize=128, shuffle=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will actually build our neural network. We will use two layers. The hidden layer will have 32 nodes, and the output layer will have 10 nodes. i.e. we will go from: `28*28 => 32 => 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(784 => 32, relu),               \u001b[90m# 25_120 parameters\u001b[39m\n",
       "  Dense(32 => 10),                      \u001b[90m# 330 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m25_450 parameters, 99.617 KiB."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "  Dense(28^2, 32, relu),\n",
    "  Dense(32, 10),\n",
    "  softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `model`, the neural network mean here? \n",
    "\n",
    "If you've worked with neural networks before you know that the solution is often not found by just one pass on the neural network. One pass happens, and a solution is generated at the output layer, then this solution is compared to the ground truth solution we already have (the columns from `y_train`), and the network goes back and adjusts its weights and parameters and then try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our neural network, we need a loss function and an accuracy function. The accuracy function is used to compare the output result from the output layer in the neural network to the groundtruth result. The loss function is used to evaluate the performance of the overall model after new weights have been recalculated at each pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(model, x, y) = (loss = Flux.logitcrossentropy(model(x), y); @show loss; loss)\n",
    "opt_state = Flux.setup(Flux.Adam(0.01), model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function to display the loss at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 2.3003216f0\n",
      "loss = 2.236171f0\n",
      "loss = 2.193145f0\n",
      "loss = 2.0871193f0\n",
      "loss = 2.0816321f0\n",
      "loss = 1.9156976f0\n",
      "loss = 1.9012768f0\n",
      "loss = 1.8663958f0\n",
      "loss = 1.8437393f0\n",
      "loss = 1.8504336f0\n",
      "loss = 1.8618894f0\n",
      "loss = 1.8176337f0\n",
      "loss = 1.8297461f0\n",
      "loss = 1.7056276f0\n",
      "loss = 1.7136507f0\n",
      "loss = 1.7228457f0\n",
      "loss = 1.7086393f0\n",
      "loss = 1.6583561f0\n",
      "loss = 1.6814754f0\n",
      "loss = 1.6937943f0\n",
      "loss = 1.6113474f0\n",
      "loss = 1.6363189f0\n",
      "loss = 1.6596433f0\n",
      "loss = 1.6165588f0\n",
      "loss = 1.6674447f0\n",
      "loss = 1.6304032f0\n",
      "loss = 1.6848623f0\n",
      "loss = 1.6961565f0\n",
      "loss = 1.5729822f0\n",
      "loss = 1.6249275f0\n",
      "loss = 1.6003265f0\n",
      "loss = 1.5836047f0\n",
      "loss = 1.6300077f0\n",
      "loss = 1.6477646f0\n",
      "loss = 1.6264858f0\n",
      "loss = 1.6021521f0\n",
      "loss = 1.5799218f0\n",
      "loss = 1.6091257f0\n",
      "loss = 1.5742059f0\n",
      "loss = 1.58317f0\n",
      "loss = 1.6077583f0\n",
      "loss = 1.6399919f0\n",
      "loss = 1.5647383f0\n",
      "loss = 1.5413212f0\n",
      "loss = 1.6449699f0\n",
      "loss = 1.5708293f0\n",
      "loss = 1.5498139f0\n",
      "loss = 1.6552484f0\n",
      "loss = 1.6397945f0\n",
      "loss = 1.595682f0\n",
      "loss = 1.5740894f0\n",
      "loss = 1.6215961f0\n",
      "loss = 1.6140271f0\n",
      "loss = 1.5927569f0\n",
      "loss = 1.5760587f0\n",
      "loss = 1.5637835f0\n",
      "loss = 1.6139097f0\n",
      "loss = 1.5534675f0\n",
      "loss = 1.6108067f0\n",
      "loss = 1.6159825f0\n",
      "loss = 1.5980164f0\n",
      "loss = 1.6232703f0\n",
      "loss = 1.604726f0\n",
      "loss = 1.569293f0\n",
      "loss = 1.6290679f0\n",
      "loss = 1.5780654f0\n",
      "loss = 1.5509425f0\n",
      "loss = 1.5650945f0\n",
      "loss = 1.615305f0\n",
      "loss = 1.5555524f0\n",
      "loss = 1.5508194f0\n",
      "loss = 1.5694495f0\n",
      "loss = 1.5576892f0\n",
      "loss = 1.5939486f0\n",
      "loss = 1.5704606f0\n",
      "loss = 1.6063402f0\n",
      "loss = 1.571359f0\n",
      "loss = 1.6104538f0\n",
      "loss = 1.5962644f0\n",
      "loss = 1.5973079f0\n",
      "loss = 1.579859f0\n",
      "loss = 1.567724f0\n",
      "loss = 1.6042571f0\n",
      "loss = 1.5581232f0\n",
      "loss = 1.5592757f0\n",
      "loss = 1.5867853f0\n",
      "loss = 1.5628496f0\n",
      "loss = 1.5504225f0\n",
      "loss = 1.590404f0\n",
      "loss = 1.5504217f0\n",
      "loss = 1.545083f0\n",
      "loss = 1.5591315f0\n",
      "loss = 1.5495516f0\n",
      "loss = 1.579665f0\n",
      "loss = 1.5893611f0\n",
      "loss = 1.5578079f0\n",
      "loss = 1.6004564f0\n",
      "loss = 1.5561814f0\n",
      "loss = 1.5529007f0\n",
      "loss = 1.5771927f0\n",
      "loss = 1.5635109f0\n",
      "loss = 1.5872736f0\n",
      "loss = 1.5811341f0\n",
      "loss = 1.5725325f0\n",
      "loss = 1.5767568f0\n",
      "loss = 1.587747f0\n",
      "loss = 1.5509615f0\n",
      "loss = 1.5367662f0\n",
      "loss = 1.542142f0\n",
      "loss = 1.6077727f0\n",
      "loss = 1.5643758f0\n",
      "loss = 1.5705992f0\n",
      "loss = 1.56839f0\n",
      "loss = 1.5860214f0\n",
      "loss = 1.5626304f0\n",
      "loss = 1.5397784f0\n",
      "loss = 1.5697232f0\n",
      "loss = 1.5651674f0\n",
      "loss = 1.5838311f0\n",
      "loss = 1.5365307f0\n",
      "loss = 1.6288033f0\n",
      "loss = 1.54993f0\n",
      "loss = 1.5395087f0\n",
      "loss = 1.60142f0\n",
      "loss = 1.5580156f0\n",
      "loss = 1.55954f0\n",
      "loss = 1.579985f0\n",
      "loss = 1.6217793f0\n",
      "loss = 1.5780575f0\n",
      "loss = 1.5777034f0\n",
      "loss = 1.5964237f0\n",
      "loss = 1.5676259f0\n",
      "loss = 1.5680549f0\n",
      "loss = 1.5781783f0\n",
      "loss = 1.5331596f0\n",
      "loss = 1.5735277f0\n",
      "loss = 1.5527227f0\n",
      "loss = 1.573087f0\n",
      "loss = 1.5528591f0\n",
      "loss = 1.5413969f0\n",
      "loss = 1.5863919f0\n",
      "loss = 1.5718026f0\n",
      "loss = 1.5750027f0\n",
      "loss = 1.5212773f0\n",
      "loss = 1.5550097f0\n",
      "loss = 1.549005f0\n",
      "loss = 1.5414824f0\n",
      "loss = 1.5765264f0\n",
      "loss = 1.5375755f0\n",
      "loss = 1.5335188f0\n",
      "loss = 1.543436f0\n",
      "loss = 1.544918f0\n",
      "loss = 1.5734429f0\n",
      "loss = 1.5255643f0\n",
      "loss = 1.5579039f0\n",
      "loss = 1.5865732f0\n",
      "loss = 1.5522097f0\n",
      "loss = 1.5743158f0\n",
      "loss = 1.5579313f0\n",
      "loss = 1.578588f0\n",
      "loss = 1.5502952f0\n",
      "loss = 1.5381031f0\n",
      "loss = 1.5934048f0\n",
      "loss = 1.5278089f0\n",
      "loss = 1.5223854f0\n",
      "loss = 1.575974f0\n",
      "loss = 1.5675442f0\n",
      "loss = 1.5355932f0\n",
      "loss = 1.5597861f0\n",
      "loss = 1.5703518f0\n",
      "loss = 1.575657f0\n",
      "loss = 1.5541326f0\n",
      "loss = 1.5485034f0\n",
      "loss = 1.5329732f0\n",
      "loss = 1.5665928f0\n",
      "loss = 1.5647883f0\n",
      "loss = 1.5694554f0\n",
      "loss = 1.5414779f0\n",
      "loss = 1.5423868f0\n",
      "loss = 1.5624052f0\n",
      "loss = 1.5603977f0\n",
      "loss = 1.5315137f0\n",
      "loss = 1.5427066f0\n",
      "loss = 1.5762502f0\n",
      "loss = 1.5292951f0\n",
      "loss = 1.5490547f0\n",
      "loss = 1.5531503f0\n",
      "loss = 1.5277998f0\n",
      "loss = 1.5650132f0\n",
      "loss = 1.5892301f0\n",
      "loss = 1.5456245f0\n",
      "loss = 1.5311435f0\n",
      "loss = 1.5335909f0\n",
      "loss = 1.5593085f0\n",
      "loss = 1.5414643f0\n",
      "loss = 1.545136f0\n",
      "loss = 1.5718533f0\n",
      "loss = 1.5540416f0\n",
      "loss = 1.5858085f0\n",
      "loss = 1.509151f0\n",
      "loss = 1.5719951f0\n",
      "loss = 1.5806649f0\n",
      "loss = 1.5630367f0\n",
      "loss = 1.5960408f0\n",
      "loss = 1.5804055f0\n",
      "loss = 1.5487075f0\n",
      "loss = 1.5464846f0\n",
      "loss = 1.5976533f0\n",
      "loss = 1.4962157f0\n",
      "loss = 1.5866753f0\n",
      "loss = 1.5525551f0\n",
      "loss = 1.5571622f0\n",
      "loss = 1.5506784f0\n",
      "loss = 1.6000248f0\n",
      "loss = 1.561908f0\n",
      "loss = 1.5294528f0\n",
      "loss = 1.5260139f0\n",
      "loss = 1.5442805f0\n",
      "loss = 1.5650605f0\n",
      "loss = 1.5712383f0\n",
      "loss = 1.56154f0\n",
      "loss = 1.5547397f0\n",
      "loss = 1.5502179f0\n",
      "loss = 1.5661448f0\n",
      "loss = 1.5556047f0\n",
      "loss = 1.5383245f0\n",
      "loss = 1.5276055f0\n",
      "loss = 1.5836726f0\n",
      "loss = 1.5706263f0\n",
      "loss = 1.548066f0\n",
      "loss = 1.5538394f0\n",
      "loss = 1.5589494f0\n",
      "loss = 1.5502131f0\n",
      "loss = 1.5496018f0\n",
      "loss = 1.552091f0\n",
      "loss = 1.5339165f0\n",
      "loss = 1.5456755f0\n",
      "loss = 1.5325992f0\n",
      "loss = 1.5193335f0\n",
      "loss = 1.5545695f0\n",
      "loss = 1.5566268f0\n",
      "loss = 1.5525275f0\n",
      "loss = 1.557995f0\n",
      "loss = 1.6128274f0\n",
      "loss = 1.5546803f0\n",
      "loss = 1.5645611f0\n",
      "loss = 1.549204f0\n",
      "loss = 1.536118f0\n",
      "loss = 1.543447f0\n",
      "loss = 1.5532308f0\n",
      "loss = 1.5305514f0\n",
      "loss = 1.5278167f0\n",
      "loss = 1.5322281f0\n",
      "loss = 1.5379747f0\n",
      "loss = 1.5464524f0\n",
      "loss = 1.5486201f0\n",
      "loss = 1.5457237f0\n",
      "loss = 1.5738912f0\n",
      "loss = 1.5413011f0\n",
      "loss = 1.5264379f0\n",
      "loss = 1.5504116f0\n",
      "loss = 1.5613302f0\n",
      "loss = 1.5524523f0\n",
      "loss = 1.5281444f0\n",
      "loss = 1.5306906f0\n",
      "loss = 1.5463167f0\n",
      "loss = 1.5254198f0\n",
      "loss = 1.5546346f0\n",
      "loss = 1.5572295f0\n",
      "loss = 1.541491f0\n",
      "loss = 1.5710247f0\n",
      "loss = 1.5325902f0\n",
      "loss = 1.5629528f0\n",
      "loss = 1.5338836f0\n",
      "loss = 1.5081376f0\n",
      "loss = 1.5649292f0\n",
      "loss = 1.5446303f0\n",
      "loss = 1.5392834f0\n",
      "loss = 1.5302321f0\n",
      "loss = 1.5553659f0\n",
      "loss = 1.5591607f0\n",
      "loss = 1.5450932f0\n",
      "loss = 1.5312675f0\n",
      "loss = 1.5479975f0\n",
      "loss = 1.5479472f0\n",
      "loss = 1.5562302f0\n",
      "loss = 1.5539384f0\n",
      "loss = 1.523634f0\n",
      "loss = 1.5136323f0\n",
      "loss = 1.540109f0\n",
      "loss = 1.5190644f0\n",
      "loss = 1.5149047f0\n",
      "loss = 1.5608473f0\n",
      "loss = 1.5245527f0\n",
      "loss = 1.5667986f0\n",
      "loss = 1.5617392f0\n",
      "loss = 1.527385f0\n",
      "loss = 1.5595549f0\n",
      "loss = 1.5453647f0\n",
      "loss = 1.5444041f0\n",
      "loss = 1.5407621f0\n",
      "loss = 1.5229642f0\n",
      "loss = 1.5566715f0\n",
      "loss = 1.5483878f0\n",
      "loss = 1.5333779f0\n",
      "loss = 1.55689f0\n",
      "loss = 1.5182263f0\n",
      "loss = 1.5349201f0\n",
      "loss = 1.5549555f0\n",
      "loss = 1.5837497f0\n",
      "loss = 1.5796705f0\n",
      "loss = 1.5192386f0\n",
      "loss = 1.5172787f0\n",
      "loss = 1.5386512f0\n",
      "loss = 1.5617712f0\n",
      "loss = 1.5430795f0\n",
      "loss = 1.5488476f0\n",
      "loss = 1.529964f0\n",
      "loss = 1.5499333f0\n",
      "loss = 1.5459315f0\n",
      "loss = 1.5777656f0\n",
      "loss = 1.5434269f0\n",
      "loss = 1.5196487f0\n",
      "loss = 1.5384043f0\n",
      "loss = 1.5400925f0\n",
      "loss = 1.5383823f0\n",
      "loss = 1.5487932f0\n",
      "loss = 1.5756645f0\n",
      "loss = 1.5593946f0\n",
      "loss = 1.5312419f0\n",
      "loss = 1.5753028f0\n",
      "loss = 1.5735227f0\n",
      "loss = 1.5500777f0\n",
      "loss = 1.5292962f0\n",
      "loss = 1.5495594f0\n",
      "loss = 1.5824138f0\n",
      "loss = 1.5323403f0\n",
      "loss = 1.5513531f0\n",
      "loss = 1.5530478f0\n",
      "loss = 1.531948f0\n",
      "loss = 1.5542209f0\n",
      "loss = 1.5108302f0\n",
      "loss = 1.5496323f0\n",
      "loss = 1.5318406f0\n",
      "loss = 1.5239073f0\n",
      "loss = 1.5371994f0\n",
      "loss = 1.5914339f0\n",
      "loss = 1.5454723f0\n",
      "loss = 1.5613723f0\n",
      "loss = 1.514389f0\n",
      "loss = 1.5549413f0\n",
      "loss = 1.5289539f0\n",
      "loss = 1.553899f0\n",
      "loss = 1.5314432f0\n",
      "loss = 1.5596604f0\n",
      "loss = 1.5141907f0\n",
      "loss = 1.5049543f0\n",
      "loss = 1.5087357f0\n",
      "loss = 1.5247108f0\n",
      "loss = 1.5428023f0\n",
      "loss = 1.529477f0\n",
      "loss = 1.5674633f0\n",
      "loss = 1.538498f0\n",
      "loss = 1.5031507f0\n",
      "loss = 1.5436883f0\n",
      "loss = 1.5238192f0\n",
      "loss = 1.5339134f0\n",
      "loss = 1.5241503f0\n",
      "loss = 1.5528152f0\n",
      "loss = 1.5343168f0\n",
      "loss = 1.5307406f0\n",
      "loss = 1.5385877f0\n",
      "loss = 1.5330638f0\n",
      "loss = 1.5557085f0\n",
      "loss = 1.5220982f0\n",
      "loss = 1.5170715f0\n",
      "loss = 1.5252045f0\n",
      "loss = 1.5154748f0\n",
      "loss = 1.546087f0\n",
      "loss = 1.5387129f0\n",
      "loss = 1.5471048f0\n",
      "loss = 1.5497265f0\n",
      "loss = 1.5281446f0\n",
      "loss = 1.556741f0\n",
      "loss = 1.5224801f0\n",
      "loss = 1.5398995f0\n",
      "loss = 1.5328834f0\n",
      "loss = 1.5245229f0\n",
      "loss = 1.5414526f0\n",
      "loss = 1.5251943f0\n",
      "loss = 1.5228872f0\n",
      "loss = 1.5983735f0\n",
      "loss = 1.5364157f0\n",
      "loss = 1.514331f0\n",
      "loss = 1.5288625f0\n",
      "loss = 1.5210836f0\n",
      "loss = 1.5353333f0\n",
      "loss = 1.5100129f0\n",
      "loss = 1.5102648f0\n",
      "loss = 1.5671308f0\n",
      "loss = 1.541639f0\n",
      "loss = 1.5556695f0\n",
      "loss = 1.546989f0\n",
      "loss = 1.5398695f0\n",
      "loss = 1.5441077f0\n",
      "loss = 1.5149684f0\n",
      "loss = 1.5063026f0\n",
      "loss = 1.5112637f0\n",
      "loss = 1.5218973f0\n",
      "loss = 1.5524008f0\n",
      "loss = 1.5547478f0\n",
      "loss = 1.5888056f0\n",
      "loss = 1.520928f0\n",
      "loss = 1.510679f0\n",
      "loss = 1.554241f0\n",
      "loss = 1.5328696f0\n",
      "loss = 1.5300537f0\n",
      "loss = 1.5455711f0\n",
      "loss = 1.5566865f0\n",
      "loss = 1.5694524f0\n",
      "loss = 1.5236913f0\n",
      "loss = 1.5170522f0\n",
      "loss = 1.5964681f0\n",
      "loss = 1.5269159f0\n",
      "loss = 1.5499457f0\n",
      "loss = 1.5132014f0\n",
      "loss = 1.5435281f0\n",
      "loss = 1.5448858f0\n",
      "loss = 1.5379814f0\n",
      "loss = 1.5204855f0\n",
      "loss = 1.4896353f0\n",
      "loss = 1.5115801f0\n",
      "loss = 1.5326481f0\n",
      "loss = 1.5348245f0\n",
      "loss = 1.5281061f0\n",
      "loss = 1.5353603f0\n",
      "loss = 1.5760232f0\n",
      "loss = 1.5182043f0\n",
      "loss = 1.5392308f0\n",
      "loss = 1.5323164f0\n",
      "loss = 1.5330055f0\n",
      "loss = 1.5317775f0\n",
      "loss = 1.5347611f0\n",
      "loss = 1.5205449f0\n",
      "loss = 1.550728f0\n",
      "loss = 1.4987303f0\n",
      "loss = 1.548102f0\n",
      "loss = 1.5489582f0\n",
      "loss = 1.5171379f0\n",
      "loss = 1.4986653f0\n",
      "loss = 1.5292214f0\n",
      "loss = 1.5200686f0\n",
      "loss = 1.5319666f0\n",
      "loss = 1.5380875f0\n",
      "loss = 1.5513251f0\n",
      "loss = 1.5685761f0\n",
      "loss = 1.5168257f0\n",
      "loss = 1.5393697f0\n",
      "loss = 1.5359601f0\n",
      "loss = 1.5487463f0\n",
      "loss = 1.5131228f0\n",
      "loss = 1.5322583f0\n",
      "loss = 1.5278713f0\n",
      "loss = 1.56346f0\n",
      "loss = 1.5188706f0\n",
      "loss = 1.5263366f0\n",
      "loss = 1.5182723f0\n",
      "loss = 1.5172709f0\n",
      "loss = 1.5447575f0\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "Flux.train!(loss, model, loader, opt_state)\n",
    "println(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now get the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset MNIST:\n",
       "  metadata  =>    Dict{String, Any} with 3 entries\n",
       "  split     =>    :test\n",
       "  features  =>    28×28×10000 Array{Float32, 3}\n",
       "  targets   =>    10000-element Vector{Int64}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = MNIST(split=:test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test_data.features\n",
    "x_test = flatten(x_test)\n",
    "test_image = model(x_test[:,1])\n",
    "argmax(test_image) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAdBJREFUaAW9wb1qlgcABtCDeToUXLRU6FB/cOtSgggFWyh0EV0EvYXUoXQpBFxCQYdAxg7egeAFlBIKKXTRJYv4UyrGwYoIhQS0Q1ChDu8QBL/4vfnCc06URVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVlMcAkLeIZt3MBzPDKbKIuyKIsJVnDcjst4ift29xQrWPd+URZlURYTLOBLPMAXmMe3+Ar/4HM73uBffGbwBOveL8qiLMpigjWsGawaHMI81nHajm08xF84jMcmi7Ioi7IYYQt/GKx510Ucwl3cNFmURVmUxT44gus4gKvYNFmURVmUxT74AZ9iC3/bXZRFWZTFjM7gisEF3LO7KIuyKIsZncNHWMNtHxZlURZlMYOPcRav8DNe+7Aoi7IoixksYh6ruGU6URZlURZ7dB5LeIFrphdlURZlsQef4BfM4TfcNr0oi7Ioi5HmsIoT2MCScaIsyqIsRjqJUwY/YcM4URZlURYjHMPvBov41XhRFmVRFiN8j6MGf+J/40VZlEVZTOkb/Gh2URZlURZT+hoHDTbwn72JsiiLshjpDr7Dpr2JsiiLspjSMpbNLsqiLMreApamPWWOWvFrAAAAAElFTkSuQmCC",
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAAAdBJREFUaAW9wb1qlgcABtCDeToUXLRU6FB/cOtSgggFWyh0EV0EvYXUoXQpBFxCQYdAxg7egeAFlBIKKXTRJYv4UyrGwYoIhQS0Q1ChDu8QBL/4vfnCc06URVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVmURVlMcAkLeIZt3MBzPDKbKIuyKIsJVnDcjst4ift29xQrWPd+URZlURYTLOBLPMAXmMe3+Ar/4HM73uBffGbwBOveL8qiLMpigjWsGawaHMI81nHajm08xF84jMcmi7Ioi7IYYQt/GKx510Ucwl3cNFmURVmUxT44gus4gKvYNFmURVmUxT74AZ9iC3/bXZRFWZTFjM7gisEF3LO7KIuyKIsZncNHWMNtHxZlURZlMYOPcRav8DNe+7Aoi7IoixksYh6ruGU6URZlURZ7dB5LeIFrphdlURZlsQef4BfM4TfcNr0oi7Ioi5HmsIoT2MCScaIsyqIsRjqJUwY/YcM4URZlURYjHMPvBov41XhRFmVRFiN8j6MGf+J/40VZlEVZTOkb/Gh2URZlURZT+hoHDTbwn72JsiiLshjpDr7Dpr2JsiiLspjSMpbNLsqiLMreApamPWWOWvFrAAAAAElFTkSuQmCC\">"
      ],
      "text/plain": [
       "28×28 reinterpret(reshape, Gray{Float32}, adjoint(::Matrix{Float32})) with eltype Gray{Float32}:\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0       …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0       …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.329412     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.870588     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.262745     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0       …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱                 ⋮         \n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0       …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0       …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0          0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = reshape(x_test[:,1],28,28)\n",
    "colorview(Gray, t1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the image we tried a few cells earlier and returned the \"not-so-great\" answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float32}:\n",
       " 1.0\n",
       " 2.1732135f-30\n",
       " 1.0297358f-12\n",
       " 6.8872825f-14\n",
       " 1.4270435f-25\n",
       " 7.2391484f-15\n",
       " 1.1186819f-17\n",
       " 5.1354913f-18\n",
       " 1.2558939f-16\n",
       " 5.4961194f-19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onefigure = x_train[:,2]\n",
    "model(onefigure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element OneHotVector(::UInt32) with eltype Bool:\n",
       " 1\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅\n",
       " ⋅"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally...\n",
    "After finishing this notebook, you should be able to:\n",
    "- prepare data to fit the format for Flux.jl\n",
    "- using a neural network with Flux.jl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
